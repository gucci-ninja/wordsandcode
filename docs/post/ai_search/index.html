<!doctype html>
<html lang="en-us">
  <head><script src="/wordsandcode/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=wordsandcode/livereload" data-no-instant defer></script>
    <title>Using FAISS for local semantic search // Words &amp; Code</title>
    <link rel="shortcut icon" href="https://pngimage.net/wp-content/uploads/2018/05/bullet-style-png-3-300x200.png" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.154.5">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Suhavi Sandhu" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/wordsandcode/css/main.min.a7dfeaabca38e15028cf2fc45c1eca64f842bd62529f0bf081a55d5270f9d37a.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Using FAISS for local semantic search">
  <meta name="twitter:description" content="Introduction This weekend I decided to play around with vector databases, since I had mentioned in my last post how they play a big role in AI-powered search. In general, AI-powered search is popping off for a good reason - it significantly improves the accuracy and relevance of search results because it understands the intention behind a search query. So much so that it has deprecated the need for stack overflow in my life.">

    <meta property="og:url" content="http://localhost:1313/wordsandcode/post/ai_search/">
  <meta property="og:site_name" content="Words & Code">
  <meta property="og:title" content="Using FAISS for local semantic search">
  <meta property="og:description" content="Introduction This weekend I decided to play around with vector databases, since I had mentioned in my last post how they play a big role in AI-powered search. In general, AI-powered search is popping off for a good reason - it significantly improves the accuracy and relevance of search results because it understands the intention behind a search query. So much so that it has deprecated the need for stack overflow in my life.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-03-16T16:48:30-07:00">
    <meta property="article:modified_time" content="2025-03-16T16:48:30-07:00">


  </head>
  <body>
    <header class="app-header">
      <a href="/wordsandcode/"><img class="app-header-avatar" src="/wordsandcode/img/avatar.jpg" alt="Suhavi Sandhu" /></a>
      <span class="app-header-title">Words &amp; Code</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/wordsandcode/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/about/">About</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/design_review_tips/">8 Tips for Effective Design Reviews</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/thanksgiving/">A-Z Tech to be thankful for in 2020</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/ai_abc/">AI ABCs</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/guardian/">Building Deterministic Guardrails for Autonomous Agents</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/elastic_cc/">Elastic Community Conference Tech Talk</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/flutter_rive_tutorial/">Flutter Web and Rive Animation Tutorial</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/archero/">How I Built an AI Assistant for my Operating System</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/open_source/">How to start contributing to open source</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/kafka_summit/">Kafka Summit Tech Talk</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/guardian_v2/">Part 2: Building Deterministic Guardrails for Autonomous Agents</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/ai_search/">Using FAISS for local semantic search</a>
             - 
          
          <a class="app-header-menu-item" href="/wordsandcode/post/arch_linux/">Your Guide to Arch Linux</a>
      </nav>
      <p>Software engineer passionate about building powerful, useful and open source systems. Thanks for dropping by! üíõ</p>
      <div class="app-header-social">
        
          <a href="https://github.com/gucci-ninja" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-github" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>github</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg>
          </a>
        
          <a href="https://twitter.com/SuhaviSandhu" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-twitter" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>twitter</title><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg>
          </a>
        
          <a href="https://www.linkedin.com/in/suhavi/" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-linkedin" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>linkedin</title><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Using FAISS for local semantic search</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          Mar 16, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          5 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <h2 id="introduction">Introduction</h2>
<p>This weekend I decided to play around with vector databases, since I had mentioned in my last post how they play a big role in AI-powered search. In general, AI-powered search is popping off for a good reason - it significantly improves the accuracy and relevance of search results because it understands the <em>intention</em> behind a search query. So much so that it has deprecated the need for stack overflow in my life.</p>
<p>AI-powered search is made possible via vector databases, which store data as vectors that have been optimized for search. Many services use cloud-based vector DBs, which come with their own set of drawbacks such as</p>
<ul>
<li>Privacy - all the data is stored on external servers</li>
<li>Latency - querying remote APIs adds additional delays</li>
<li>Recurring cost - cloud-based solution takes cloud based money</li>
</ul>
<p>That‚Äôs why I‚Äôm setting up a <strong>local</strong> vector database for searching. Not because I‚Äôm broke, because I value privacy, duh. I‚Äôll be using FAISS, which is Facebook‚Äôs AI Similarity Search, an open-source library that offers multiple semantic search algorithms.</p>
<h2 id="what-is-faiss--how-does-it-work"><strong>What is FAISS &amp; How Does It Work?</strong></h2>
<p>FAISS takes in data that has been transformed into vectors and performs search on them, with vectors of any size, as long as they fit within the RAM specified on your machine. It‚Äôs written in C++ and has wrappers for Python‚Äôs numpy library. You can run it on a CPU or a GPU -  while GPU will perform better, I will be using the CPU version.</p>
<h3 id="key-benefits-of-faiss">Key Benefits of FAISS:</h3>
<ul>
<li><strong>Optimized for speed</strong>: FAISS uses advanced indexing techniques for fast lookups.</li>
<li><strong>Scalable</strong>: It can handle millions of vectors with reasonable memory usage.</li>
<li><strong>Supports various indexing methods</strong>: Flat indexes, IVF (Inverted File Index), PQ (Product Quantization), and HNSW (Hierarchical Navigable Small World) for performance tuning.</li>
</ul>
<p>By leveraging FAISS, we can create a <strong>local AI-powered search engine</strong> that processes embeddings from text, images, or any structured data.</p>
<h2 id="setting-up-local-semantic-search-system">Setting up local semantic search system</h2>
<h3 id="step-1-installing-faiss--choosing-an-embedding-model">Step 1: Installing FAISS &amp; Choosing an Embedding Model</h3>
<p>While FAISS takes care of indexing and searching a vector database, it requires the input to the database to be somewhat standardized. For that we will use an embedding model from sentence-transformers, a Python library designed for efficiently computing dense vector embeddings of sentences, paragraphs or documents using Transformer-based models. It comes with a a pre-trained model called <code>all-MiniLM-L6-v2</code>.</p>
<h3 id="prerequisites">Prerequisites:</h3>
<ul>
<li>Python 3.8+</li>
<li>FAISS library</li>
<li>Sentence-transformers (for text embeddings)</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install faiss-cpu sentence-transformers
</span></span></code></pre></div><p>If you‚Äôre using GPU</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install faiss-gpu sentence-transformers
</span></span></code></pre></div><h3 id="step-2-indexing-our-dataset">Step 2: Indexing our dataset</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> faiss
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> SentenceTransformer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SentenceTransformer(<span style="color:#e6db74">&#34;all-MiniLM-L6-v2&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>documents <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;This is my first note&#34;</span>, <span style="color:#e6db74">&#34;AI-powered search is cool&#34;</span>, <span style="color:#e6db74">&#34;Privacy matters&#34;</span>]
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(documents, convert_to_numpy<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a FAISS index</span>
</span></span><span style="display:flex;"><span>dimension <span style="color:#f92672">=</span> embeddings<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> faiss<span style="color:#f92672">.</span>IndexFlatL2(dimension)
</span></span><span style="display:flex;"><span>index<span style="color:#f92672">.</span>add(embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save index to disk</span>
</span></span><span style="display:flex;"><span>faiss<span style="color:#f92672">.</span>write_index(index, <span style="color:#e6db74">&#34;search_index.bin&#34;</span>)
</span></span></code></pre></div><p>This will create a .bin file, which was a mere 8KB for me and took less than 3 seconds to generate. Ideally this will be generated when new data is being added, not during each search.</p>
<h3 id="step-3-build-a-simple-search-interface">Step 3: Build a simple search interface</h3>
<p>Now that we have indexed a little bit of data, we can build a function to retrieve relevant results. The following code takes in a textual query and returns the top k documents that are most similar to our query.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">search</span>(query, top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>    query_embedding <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode([query], convert_to_numpy<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    distances, indices <span style="color:#f92672">=</span> index<span style="color:#f92672">.</span>search(query_embedding, top_k)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [(documents[i], distances[<span style="color:#ae81ff">0</span>][j]) <span style="color:#66d9ef">for</span> j, i <span style="color:#f92672">in</span> enumerate(indices[<span style="color:#ae81ff">0</span>])]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example query</span>
</span></span><span style="display:flex;"><span>print(search(<span style="color:#e6db74">&#34;AI search&#34;</span>), <span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><p>This output should look something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">[</span>
</span></span><span style="display:flex;"><span>	<span style="color:#f92672">(</span><span style="color:#e6db74">&#39;AI-powered search is cool&#39;</span>, np.float32<span style="color:#f92672">(</span>0.43699586<span style="color:#f92672">))</span>,
</span></span><span style="display:flex;"><span>	<span style="color:#f92672">(</span><span style="color:#e6db74">&#39;Privacy matters&#39;</span>, np.float32<span style="color:#f92672">(</span>1.7605863<span style="color:#f92672">))</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">]</span>
</span></span></code></pre></div><p>This can easily be made into a CLI tool or even a web app if you want a nicer way to interact with the search system. One use case I can see is to run it against a notes app that stores documents in a straightforward way (JSON, CSV, etc), indexing whenever a new note is added, and then being able to search for data points such as ‚Äòwhat i did on X day‚Äô, ‚Äòdid I do Y already‚Äô.</p>
<h2 id="benchmarking-local-vs-cloud-ai-search">Benchmarking Local vs Cloud AI search</h2>
<p>As I mentioned earlier, local vector databases provide better performance in theory, so I decided to put it to the test. I‚Äôll be benchmarking against OpenAI‚Äôs API-based search since I‚Äôve used it before. The code for benchmarking will be available in <a href="https://github.com/gucci-ninja/local-ai-search">this Github repo</a>, and ‚Äôm not ashamed to admit Chat GPT wrote 90% of it. Below are the results from the benchmarking. I used the same 3 documents to index.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>üîç **Search Results**
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>üìå **FAISS <span style="color:#f92672">(</span>Local Search<span style="color:#f92672">)</span>**
</span></span><span style="display:flex;"><span>1. Document ID <span style="color:#ae81ff">1</span> <span style="color:#f92672">(</span>Distance: 0.4905<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>2. Document ID <span style="color:#ae81ff">2</span> <span style="color:#f92672">(</span>Distance: 1.7902<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>3. Document ID <span style="color:#ae81ff">0</span> <span style="color:#f92672">(</span>Distance: 1.8825<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>‚è± Time taken: 0.0025 sec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>üåç **OpenAI API <span style="color:#f92672">(</span>Cloud Search<span style="color:#f92672">)</span>**
</span></span><span style="display:flex;"><span>1. Document ID <span style="color:#ae81ff">1</span> <span style="color:#f92672">(</span>Similarity: 0.8692<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>2. Document ID <span style="color:#ae81ff">2</span> <span style="color:#f92672">(</span>Similarity: 0.7573<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>3. Document ID <span style="color:#ae81ff">0</span> <span style="color:#f92672">(</span>Similarity: 0.7423<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>‚è± Time taken: 0.7617 sec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>üìä **Comparison Summary:**
</span></span><span style="display:flex;"><span>- FAISS is **307.23x** faster.
</span></span></code></pre></div><p>Each time I ran the benchmarking script, FAISS came out on top, taking around 300-500 times less time than OpenAI. I haven‚Äôt tried with some other popular models such Anthropic or DeepSeek.</p>
<p>Here is the overall comparison of locally using FAISS vs OpenAI API</p>
<table>
  <thead>
      <tr>
          <th><strong>Metric</strong></th>
          <th><strong>FAISS (Local)</strong></th>
          <th><strong>OpenAI API Search</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Query Speed</td>
          <td><strong>Milliseconds</strong></td>
          <td><strong>Seconds (API latency)</strong></td>
      </tr>
      <tr>
          <td>Privacy</td>
          <td><strong>High</strong>¬†(Local processing)</td>
          <td><strong>Low</strong>¬†(Data sent to API)</td>
      </tr>
      <tr>
          <td>Cost</td>
          <td><strong>Free</strong></td>
          <td><strong>Pay-per-query</strong></td>
      </tr>
      <tr>
          <td>Accuracy</td>
          <td><strong>Comparable</strong></td>
          <td><strong>Depends on API model</strong></td>
      </tr>
  </tbody>
</table>
<h2 id="conclusion">Conclusion</h2>
<p>Doing this activity shows that a cloud-based API solution is not always needed and you can get comparable, sometimes better results, using a local solution.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
